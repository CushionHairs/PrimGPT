# web_utils.py
import logging
from typing import Optional, Tuple

import requests
from newspaper import Article, ArticleException
from bs4 import BeautifulSoup
from selenium import webdriver 
from selenium.webdriver.chrome.service import Service 
from selenium.webdriver.chrome.options import Options 
from webdriver_manager.chrome import ChromeDriverManager
import streamlit as st
import time 

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Constants ---
REQUEST_TIMEOUT = 15 # seconds
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36" # Common user agent

def sanitize_text(text: str) -> str:
    return text.replace("~", "&#126;")

def fetch_main_content(url: str) -> Tuple[Optional[str], int]:
    try:
        headers = {"User-Agent": USER_AGENT}
        article = Article(url, request_timeout=REQUEST_TIMEOUT, headers=headers)
        article.download()
        article.parse()

        main_text = sanitize_text(article.text)
        title = sanitize_text(article.title)

        if not main_text:
            logger.warning(f"Newspaper3k could not extract text from URL: {url}")
            st.warning("Could not extract main content from the webpage.")
            return None, 204  # No Content

        logger.info(f"Successfully extracted content from: {url}")
        output = f"### ğŸ“° **Title:** {title}\n\n---\n\nğŸ“œ **Content:**\n{main_text}"
        return output, 200

    except ArticleException as e:
        logger.error(f"Newspaper3k failed for URL {url}: {e}")
        st.error(f"Failed to process the webpage: {e}")
        return None, 500
    except requests.exceptions.RequestException as e:
        logger.error(f"Network error fetching URL {url}: {e}")
        st.error(f"Could not connect to the URL: {e}")
        return None, 503
    except Exception as e:
        logger.exception(f"Unexpected error fetching content from {url}: {e}")
        st.error(f"An unexpected error occurred: {e}")
        return None, 500
    
def fetch_main_content_bs4(url: str) -> Tuple[Optional[str], int]:
    try:
        # Set request headers
        headers = {"User-Agent": USER_AGENT}

        # Fetch webpage content
        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
        response.raise_for_status()

        # Parse HTML content
        soup = BeautifulSoup(response.text, 'html.parser')

        # Extract title
        title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else 'No Title Found'
        title = sanitize_text(title)

        # Attempt to extract main content
        # First attempt: Look for <article> tags, common for news/blogs
        article_tag = soup.find('article')
        if article_tag:
            paragraphs = article_tag.find_all('p')
        else:
            # Fallback: Extract content from all <p> tags within <body>
            paragraphs = soup.body.find_all('p') if soup.body else []

        # Combine paragraphs into readable text
        main_text = '\n\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        main_text = sanitize_text(main_text)

        if not main_text.strip():
            logger.warning(f"BeautifulSoup could not extract meaningful text from URL: {url}")
            st.warning("Could not extract main content from the webpage.")
            return None, 204  # No Content

        logger.info(f"Successfully extracted content using BeautifulSoup from: {url}")

        output = f"### ğŸ“° **Title:** {title}\n\n---\n\nğŸ“œ **Content:**\n{main_text}"
        return output, 200

    except requests.exceptions.RequestException as e:
        logger.error(f"Network error fetching URL {url}: {e}")
        st.error(f"Could not connect to the URL: {e}")
        return None, 503

    except Exception as e:
        logger.exception(f"Unexpected error fetching content from {url}: {e}")
        st.error(f"An unexpected error occurred: {e}")
        return None, 500
    
def fetch_main_content_selenium(url: str, wait_time: int = 5) -> Tuple[Optional[str], int]:
    try:
        # Selenium ì˜µì…˜ ì„¤ì • (headless ëª¨ë“œ ì‚¬ìš©, ë¸Œë¼ìš°ì € ì°½ ë„ìš°ì§€ ì•ŠìŒ)
        chrome_options = Options()
        chrome_options.add_argument("--headless=new")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("window-size=1920,1080")
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                                    'AppleWebKit/537.36 (KHTML, like Gecko) '
                                    'Chrome/119.0.0.0 Safari/537.36')

        # ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™” (ìë™ ì„¤ì¹˜)
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)

        # ì›¹ í˜ì´ì§€ ì ‘ì†
        logger.info(f"Fetching URL with Selenium: {url}")
        driver.get(url)

        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (í•„ìš”í•œ ê²½ìš° wait_time ì¡°ì • ê°€ëŠ¥)
        time.sleep(wait_time)  # ë™ì  ì½˜í…ì¸  ë¡œë”©ì„ ìœ„í•´ ì¶©ë¶„íˆ ê¸°ë‹¤ë ¤ì¤ë‹ˆë‹¤.

        # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        html = driver.page_source

        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(html, 'html.parser')

        # íƒ€ì´í‹€ ì¶”ì¶œí•˜ê¸°
        title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "No Title Found"
        title = sanitize_text(title)

        # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œí•˜ê¸° (<article> íƒœê·¸ ìš°ì„ )
        article_tag = soup.find('article')
        if article_tag:
            paragraphs = article_tag.find_all('p')
        else:
            # <article> íƒœê·¸ ì—†ìœ¼ë©´ <body> ë‚´ ëª¨ë“  <p> íƒœê·¸ ì´ìš©
            paragraphs = soup.body.find_all('p') if soup.body else []

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        main_text = '\n\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        main_text = sanitize_text(main_text)
        
        # ë³¸ë¬¸ì´ ë¹ˆ ê²½ìš° ì²˜ë¦¬
        if not main_text.strip():
            logger.warning(f"Selenium could not extract meaningful text from URL: {url}")
            st.warning("Could not extract main content from the webpage using Selenium.")
            return None, 204  # No Content

        # ì„±ê³µì ìœ¼ë¡œ ì½˜í…ì¸  ê°€ì ¸ì™”ì„ ë•Œ
        output = f"### ğŸ“° **Title:** {title}\n\n---\n\nğŸ“œ **Content:**\n{main_text}"
        logger.info(f"Successfully extracted content using Selenium from: {url}")
        return output, 200

    except Exception as e:
        logger.exception(f"Unexpected error fetching content with Selenium from {url}: {e}")
        st.error(f"An unexpected error occurred: {e}")
        return None, 500

    finally:
        # ë“œë¼ì´ë²„ ì¢…ë£Œ
        driver.quit()